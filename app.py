# -*- coding: utf-8 -*-
"""Lab 4 Creating a 3D Image Composer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L-BwABaD5hCvfsqrPykDy2J7HxBIAaVO
"""

# !pip install torch torchvision torchaudio
# !pip install gradio
# !pip install opencv-python

import cv2
import torch
import numpy as np
import gradio as gr
from PIL import Image
import matplotlib.pyplot as plt
import torchvision.transforms as T
from torchvision import models, transforms

"""Segment human figure from an image

> Choose an image containing a person.
Utilize a pre-trained semantic segmentation model to generate a mask of the person.
Apply the mask to isolate the person from the background.
Save the segmented person image with transparency (alpha channel) for overlaying purposes.
"""

# Load pretrained DeepLabV3 model
model = models.segmentation.deeplabv3_resnet101(pretrained=True)
model.eval()

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def extract_human_figure(image):
    input_tensor = transform(image).unsqueeze(0)

    with torch.no_grad():
        output = model(input_tensor)['out'][0]

    # Get class predictions
    output_predictions = output.argmax(0).byte().cpu().numpy()

    # Create mask for class 15 (human)
    mask = (output_predictions == 15).astype(np.uint8) * 255
    image_np = np.array(image)

    # Create an output with an alpha channel
    result = np.zeros((image_np.shape[0], image_np.shape[1], 4), dtype=np.uint8)

    # Copy RGB channels from original image where the mask is non-zero
    result[:, :, :3] = image_np
    result[:, :, 3] = mask

    return result

"""Insert segmented human figure into background images.

> Obtain to an external site. the provided left and right stereoscopic images.
Decide on the depth level at which you want to insert the person (close, medium, far).
Adjust the horizontal position of the person in the left and right images to create the desired disparity.
Overlay the person onto the images at the calculated positions.
"""

def overlay_figure_on_background(figure, background, x, y):
    """
    Place the segmented human figure on the background image.
    figure: segemented human figure with deeplabv3_resnet101
    background: background image
    x: horizontal position on the background
    y: vertical position on the background
    """

    # Split into color and alpha channels
    b, g, r, a = cv2.split(figure)

    # Create a 3-channel BGR image for the PNG
    overlay_color = cv2.merge((b, g, r))

    # Create mask using the alpha channel
    mask = cv2.merge((a, a, a)) / 255.0
    print(figure.shape)
    # Overlay the PNG onto the background
    background_section = background[y:y+figure.shape[0], x:x+figure.shape[1]]

    blended = (overlay_color * mask + background_section * (1 - mask)).astype(np.uint8)

    # Put the blended part back into the background
    background[y:y+figure.shape[0], x:x+figure.shape[1]] = blended

    return background

def resize_figure(figure):
  """
  Resize the figure to a fixed height so the original photo size won't influence the insertion.
  """
  height, width = figure.shape[:2]
  resized_height = 400
  return cv2.resize(figure, (int(resized_height * width / height), resized_height))

def insert_figure(figure, left, right, depth):
    """
    Insert the figure to different positions on the background images.
    """
    if depth == 1:  # Close -> Large disparity
        disparity = 10
    elif depth == 2:  # Medium -> Medium disparity
        disparity = 8
    elif depth == 3:  # Far -> Small disparity
        disparity = 4

    # Calculate the x positions for left and right images
    # Assuming the figure is centered vertically
    figure_height, figure_width = figure.shape[:2]
    x_left = (left.shape[1] - figure_width) // 2 - 100
    x_right = x_left - disparity
    y = 300

    # Overlay the figure onto the background images
    left_image = overlay_figure_on_background(figure, left.copy(), x_left, y)
    right_image = overlay_figure_on_background(figure, right.copy(), x_right, y)

    return left_image, right_image

"""Test different depth from close/medium/far away to test effect.

> Define disparity values for each depth level.
Experiment with different disparities to see how they affect depth perception.
Optionally, scale the person's image to enhance the depth illusion.

Combine the green channel from the left and the red channel from the right to create 3D effect.

> Separate the color channels of the left and right images.
Merge the red channel from the left image with the green and blue channels from the right image.
Save the combined image as your anaglyph output.
"""

def create_anaglyph(left_image, right_image):
    """
    Combine the green channel from the left and the red channel from the right to create 3D effect.
    """
    left_image = np.array(left_image)
    right_image = np.array(right_image)

    # Separate color channels
    left_red, left_green, left_blue = cv2.split(left_image)
    right_red, right_green, right_blue = cv2.split(right_image)

    # Merge channels to create anaglyph
    anaglyph = cv2.merge((left_red, right_green, right_blue))

    return anaglyph

def upload_image(figure, depth):
  """
  Upload the photo with a person to create 3D effect for.
  """

  if depth == "Close":
    mode = 1
  elif depth == "Medium":
    mode = 2
  else:
    mode = 3
  segmented_figure = extract_human_figure(figure)
  background1= cv2.imread("left.jpg")
  background2= cv2.imread("right.jpg")
  segmented_figure = resize_figure(segmented_figure)
  left, right = insert_figure(segmented_figure, background1, background2, mode)
  anaglyph = create_anaglyph(left, right)
  return anaglyph

"""> Set up a Gradio interface that allows users to:
* Upload a person image.
* Select the depth level (close, medium, far).
* Adjust the position of the person in the scene.
"""

input_image = gr.Image()
dropdown = gr.Dropdown(choices=["Close","Medium","Far"], label="Choose a scale")
output_image = gr.Image()

iface = gr.Interface(
    fn=upload_image,
    inputs=[
        input_image, dropdown
    ],
    outputs=output_image,
    title="Anaglyph",
    description="Upload an image, select depth, and view the segmented image."
)

iface.launch(debug=True)